{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction Using Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thresholding Numerical Feature Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### You have a set of numerical features and want to remove those with low variance (i.e., likely containing little information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 1.4, 0.2],\n",
       "       [4.9, 1.4, 0.2],\n",
       "       [4.7, 1.3, 0.2]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "# Create features and target\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "# Create thresholder\n",
    "thresholder = VarianceThreshold(threshold=.5)\n",
    "# Create high variance feature matrix\n",
    "features_high_variance = thresholder.fit_transform(features)\n",
    "# View high variance feature matrix\n",
    "features_high_variance[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68112222, 0.18871289, 3.09550267, 0.57713289])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # View variances\n",
    "thresholder.fit(features).variances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load library\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Standardize feature matrix\n",
    "scaler = StandardScaler()\n",
    "features_std = scaler.fit_transform(features)\n",
    "# Caculate variance of each feature\n",
    "selector = VarianceThreshold()\n",
    "selector.fit(features_std).variances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thresholding Binary Feature Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load library\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "# Create feature matrix with:\n",
    "# Feature 0: 80% class 0\n",
    "# Feature 1: 80% class 1\n",
    "# Feature 2: 60% class 0, 40% class 1\n",
    "features = [[0, 1, 0],\n",
    "[0, 1, 1],\n",
    "[0, 1, 0],\n",
    "[0, 1, 1],[1, 0, 0]]\n",
    "# Run threshold by variance\n",
    "thresholder = VarianceThreshold(threshold=(.75 * (1 - .75)))\n",
    "thresholder.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run threshold by variance\n",
    "thresholder = VarianceThreshold(threshold=(.75 * (1 - .75)))\n",
    "thresholder.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Highly Correlated Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use a correlation matrix to check for highly correlated features. If highly correlated features exist, consider dropping one of the correlated features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  2\n",
      "0  1  1\n",
      "1  2  0\n",
      "2  3  1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create feature matrix with two highly correlated features\n",
    "features = np.array([\n",
    "    [1, 1, 1],\n",
    "    [2, 2, 0],\n",
    "    [3, 3, 1],\n",
    "    [4, 4, 0],\n",
    "    [5, 5, 1],\n",
    "    [6, 6, 0],\n",
    "    [7, 7, 1],\n",
    "    [8, 7, 0],\n",
    "    [9, 7, 1]\n",
    "])\n",
    "\n",
    "# Convert feature matrix into DataFrame\n",
    "dataframe = pd.DataFrame(features)\n",
    "\n",
    "# Create correlation matrix\n",
    "corr_matrix = dataframe.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper_triangle = np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "upper = corr_matrix.where(upper_triangle)\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "# Drop features\n",
    "dataframe_dropped = dataframe.drop(dataframe.columns[to_drop], axis=1)\n",
    "\n",
    "# Display first 3 rows of the modified DataFrame\n",
    "print(dataframe_dropped.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features: 3\n",
      "Reduced number of features: 2\n",
      "Selected features indices: [1 2]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Assuming you have your features and target variables defined elsewhere\n",
    "# Here, I'll provide dummy data for illustration purposes\n",
    "\n",
    "# Create feature matrix with two highly correlated features\n",
    "features = np.array([[1, 1, 1],\n",
    "                     [2, 2, 0],\n",
    "                     [3, 3, 1],\n",
    "                     [4, 4, 0],\n",
    "                     [5, 5, 1],\n",
    "                     [6, 6, 0],\n",
    "                     [7, 7, 1],\n",
    "                     [8, 7, 0],\n",
    "                     [9, 7, 1]])\n",
    "\n",
    "# Dummy target variable (assuming it should have the same number of samples)\n",
    "target = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0])\n",
    "\n",
    "# Check if the number of samples in features and target are consistent\n",
    "if len(features) != len(target):\n",
    "    raise ValueError(\"Number of samples in features and target are inconsistent\")\n",
    "\n",
    "# Create SelectKBest object with chi-squared statistics\n",
    "chi2_selector = SelectKBest(chi2, k=2)\n",
    "\n",
    "# Fit the SelectKBest object with the features and target\n",
    "features_kbest = chi2_selector.fit_transform(features, target)\n",
    "\n",
    "# Show results\n",
    "print(\"Original number of features:\", features.shape[1])\n",
    "print(\"Reduced number of features:\", features_kbest.shape[1])\n",
    "print(\"Selected features indices:\", chi2_selector.get_support(indices=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Recursively Eliminating Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00850799,  0.7031277 , -0.84959297],\n",
       "       [-1.07500204,  2.56148527, -0.5924624 ],\n",
       "       [ 1.37940721, -1.77039484,  1.61514448],\n",
       "       ...,\n",
       "       [-0.80331656, -1.60648007, -0.08279018],\n",
       "       [ 0.39508844, -1.34564911,  1.20679839],\n",
       "       [-0.55383035,  0.82880112,  0.38295212]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "import warnings\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn import datasets, linear_model\n",
    "# Suppress an annoying but harmless warning\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"scipy\",message=\"^internal gelsd\")\n",
    "# Generate features matrix, target vector, and the true coefficients\n",
    "features, target = make_regression(n_samples = 10000,\n",
    "n_features = 100,\n",
    "n_informative = 2,\n",
    "random_state = 1)\n",
    "# Create a linear regression\n",
    "ols = linear_model.LinearRegression()\n",
    "# Recursively eliminate features\n",
    "rfecv = RFECV(estimator=ols, step=1, scoring=\"neg_mean_squared_error\")\n",
    "rfecv.fit(features, target)\n",
    "rfecv.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of best features\n",
    "rfecv.n_features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False,  True, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False,  True, False, False,  True, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which categories are best\n",
    "rfecv.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
